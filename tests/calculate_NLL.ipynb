{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "em2aVoafL-ML"
   },
   "source": [
    "# Para ejecutar en Google Colab en Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qJhAwN4z7jJ9",
    "outputId": "3644d6f4-9065-4616-d2dc-7ae1bbd255b2"
   },
   "outputs": [],
   "source": [
    "# Montamos el Drive al Notebook\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pnCMtpvc7n1s",
    "outputId": "54058795-981a-4e87-9b02-acf1e9b6bca6"
   },
   "outputs": [],
   "source": [
    "# Verificamos el directorio en el que nos encontramos\n",
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OoAkWfZL7oCL",
    "outputId": "0a28d0e4-3381-477e-e758-aabbc1cb7657"
   },
   "outputs": [],
   "source": [
    "# Cambiamos de directorio al Drive\n",
    "# TODO: change path\n",
    "import os\n",
    "os.chdir(\"drive/My Drive/PruebasCOLAB4/trajpred-bdl/tests\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hYovpqgdMNjU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GM2YmtlMTXp"
   },
   "source": [
    "# Inicio de CÃ³digo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hOKb3HV6YKm"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "import sys,os,logging, argparse\n",
    "''' TF_CPP_MIN_LOG_LEVEL\n",
    "0 = all messages are logged (default behavior)\n",
    "1 = INFO messages are not printed\n",
    "2 = INFO and WARNING messages are not printeds\n",
    "3 = INFO, WARNING, and ERROR messages are not printed\n",
    "'''\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "sys.path.append('../bayesian-torch')\n",
    "sys.path.append('..')\n",
    "\n",
    "import math,numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "# Local models\n",
    "from models.bayesian_models_gaussian_loss import lstm_encdec_MCDropout, lstm_encdec, lstm_encdec_variational\n",
    "from utils.datasets_utils import Experiment_Parameters, setup_loo_experiment, traj_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DWeQ9p0W0z_h"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(levelname)s: %(message)s',level=20)\n",
    "# GPU\n",
    "if torch.cuda.is_available():\n",
    "    logging.info(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYyijVou6YKq"
   },
   "outputs": [],
   "source": [
    "# Load the default parameters\n",
    "experiment_parameters = Experiment_Parameters(add_kp=False,obstacles=False)\n",
    "\n",
    "dataset_dir   = \"../datasets/\"\n",
    "dataset_names = ['eth-hotel','eth-univ','ucy-zara01','ucy-zara02','ucy-univ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLjjjcNCxLZs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y3UPx302LQVc",
    "outputId": "0aceac8c-cc3b-40c4-b7dc-2a85fc348e3d"
   },
   "outputs": [],
   "source": [
    "idTest        = 2\n",
    "batch_size     = 64 #16\n",
    "pickle        = False\n",
    "num_ensembles = 5\n",
    "\n",
    "# Load the dataset and perform the split\n",
    "training_data, validation_data, test_data, test_homography = setup_loo_experiment('ETH_UCY',dataset_dir,dataset_names,idTest,experiment_parameters,pickle_dir='../pickle',use_pickled_data=pickle)\n",
    "\n",
    "# Creamos el dataset para torch\n",
    "train_data = traj_dataset(training_data['obs_traj_rel'], training_data['pred_traj_rel'],training_data['obs_traj'], training_data['pred_traj'])\n",
    "val_data = traj_dataset(validation_data['obs_traj_rel'], validation_data['pred_traj_rel'],validation_data['obs_traj'], validation_data['pred_traj'])\n",
    "test_data = traj_dataset(test_data['obs_traj_rel'], test_data['pred_traj_rel'], test_data['obs_traj'], test_data['pred_traj'])\n",
    "\n",
    "# Form batches\n",
    "batched_train_data = torch.utils.data.DataLoader( train_data, batch_size = batch_size, shuffle=False)\n",
    "batched_val_data =  torch.utils.data.DataLoader( val_data, batch_size = batch_size, shuffle=False)\n",
    "batched_test_data =  torch.utils.data.DataLoader( test_data, batch_size = batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBOvcFgWlpsE"
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pv7T3NRgj1i-",
    "outputId": "66c50af0-56e8-4a53-ffbb-6bc5a6255902"
   },
   "outputs": [],
   "source": [
    "# Instanciamos el modelo deterministico\n",
    "model = lstm_encdec(2,128,256,2)\n",
    "model.to(device)\n",
    "\n",
    "# el modelo entrenado se carga durante las iteracciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5tPHBpSOXAOl",
    "outputId": "338c206a-4fb7-4c53-a82c-042f92f711f5"
   },
   "outputs": [],
   "source": [
    "# Instanciamos el modelo dropout\n",
    "dropout_rate = 0.6 #0.2\n",
    "\n",
    "# Instanciamos el modelo\n",
    "model_drop = lstm_encdec_MCDropout(2,128,256,2, dropout_rate = dropout_rate)\n",
    "model_drop.to(device)\n",
    "\n",
    "model_drop.load_state_dict(torch.load(\"../training_checkpoints/model_dropout_\"+str(idTest)+\".pth\",map_location=torch.device('cpu')))\n",
    "model_drop.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "py3TvUY2_gvo",
    "outputId": "d76924e2-588b-4590-e8e4-92349e5252ce"
   },
   "outputs": [],
   "source": [
    "# Instanciamos el modelo variational\n",
    "prior_mu = 0.0\n",
    "prior_sigma = 1.0\n",
    "posterior_mu_init = 0.0\n",
    "posterior_rho_init = -5 #-3.0 # 0.006715348489117967 # 0.01814992791780978 # 0.04858735157374196\n",
    "\n",
    "# Model\n",
    "model_var = lstm_encdec_variational(2,128,256,2,prior_mu,prior_sigma,posterior_mu_init,posterior_rho_init)\n",
    "model_var.to(device)\n",
    "\n",
    "model_var.load_state_dict(torch.load(\"../training_checkpoints/model_variational_\"+str(idTest)+\".pth\",map_location=torch.device('cpu')))\n",
    "#model_var.load_state_dict(torch.load(\"model_variational/model_variational_\"+str(idTest)+\".pth\"))\n",
    "model_var.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UigrovOko380"
   },
   "outputs": [],
   "source": [
    "def Gaussian2D(outputs, targets, sigmas):\n",
    "    '''\n",
    "    Computes the likelihood of predicted locations under a bivariate Gaussian distribution\n",
    "    params:\n",
    "    outputs: Torch variable containing tensor of shape [128, 12, 2]\n",
    "    targets: Torch variable containing tensor of shape [128, 12, 2]\n",
    "    sigmas:  Torch variable containing tensor of shape [128, 12, 3]\n",
    "    '''\n",
    "    #print(outputs.shape, targets.shape, sigmas.shape)\n",
    "\n",
    "    # Extract mean, std devs and correlation\n",
    "    mux, muy, sx, sy, corr = outputs[0], outputs[1], sigmas[0], sigmas[1], sigmas[2]\n",
    "\n",
    "    # Exponential to get a positive value for std dev\n",
    "    sx = np.exp(sx)\n",
    "    sy = np.exp(sy)\n",
    "    # tanh to get a value between [-1, 1] for correlation\n",
    "    corr = np.tanh(corr)\n",
    "    #mux, muy, sx, sy, corr = getCoef(outputs)s\n",
    "\n",
    "    # Compute factors\n",
    "    #normx = targets[:, :, 0] - mux\n",
    "    #normy = targets[:, :, 1] - muy\n",
    "    normx = targets[0] - mux\n",
    "    normy = targets[1] - muy\n",
    "    sxsy = sx * sy\n",
    "    z = (normx/sx)**2 + (normy/sy)**2 - 2*((corr*normx*normy)/sxsy)\n",
    "    negRho = 1 - corr**2\n",
    "\n",
    "    # Numerator\n",
    "    result = np.exp(-z/(2*negRho))\n",
    "    # Normalization factor\n",
    "    denom = 2 * np.pi * (sxsy * np.sqrt(negRho))\n",
    "\n",
    "    # Final PDF calculation\n",
    "    result = result / denom\n",
    "\n",
    "    # Numerical stability\n",
    "    epsilon = 1e-20\n",
    "    return max(result, epsilon)\n",
    "\n",
    "#    result = -torch.log(torch.clamp(result, min=epsilon))\n",
    "\n",
    "    # Compute the loss across all frames and all nodes\n",
    "#    loss = result.sum()/np.prod(result.shape)\n",
    "\n",
    "#    return(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UytaM3IRJss"
   },
   "source": [
    "### Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P0IbFtkqLQYO",
    "outputId": "a5661a20-4510-497b-c8ac-7831aaf9508b"
   },
   "outputs": [],
   "source": [
    "### Caso Train ###\n",
    "import time\n",
    "from path_prediction.obstacles import image_to_world_xy\n",
    "from scipy.stats import multivariate_normal\n",
    "import csv\n",
    "\n",
    "#from kde_nll import *\n",
    "\n",
    "#ind_sample = 1\n",
    "#bck = np.load('background.npy')\n",
    "#bck = plt.imread(os.path.join(dataset_dir,dataset_names[idTest],'reference.png'))\n",
    "\n",
    "#seed = 9\n",
    "# Agregamos la semilla\n",
    "#torch.manual_seed(seed)\n",
    "#torch.cuda.manual_seed(seed)\n",
    "\n",
    "nll_batch = 0\n",
    "# Testing\n",
    "for batch_idx, (datarel_test, targetrel_test, data_test, target_test) in enumerate(batched_train_data):\n",
    "    \n",
    "    #for ind_sample in range(data_test.shape[0]):\n",
    "\n",
    "    list_pred = []\n",
    "    list_sigmas = []\n",
    "    # prediction\n",
    "    for ind in range(num_ensembles):\n",
    "\n",
    "        # Cargamos el Modelo\n",
    "        model.load_state_dict(torch.load(\"../training_checkpoints/model_deterministic_\"+str(ind)+\"_\"+str(idTest)+\".pth\",map_location=torch.device('cpu')))\n",
    "        #model.load_state_dict(torch.load(\"model_deterministic/model_deterministic_\"+str(ind)+\"_\"+str(idTest)+\".pth\"))\n",
    "#        model.load_state_dict(torch.load(\"model_deterministic/model_deterministic_0_2.pth\",map_location=torch.device('cpu')))\n",
    "        model.eval()\n",
    "\n",
    "        # Modelo a predecir\n",
    "        #pred, sigmas  = model_drop.predict(datarel_test, dim_pred=12)\n",
    "        #pred, kl, sigmas  = model_var.predict(datarel_test, dim_pred=12)\n",
    "        pred, sigmas = model.predict(datarel_test, dim_pred=12)\n",
    "\n",
    "\n",
    "        list_pred.append(pred)\n",
    "        list_sigmas.append(sigmas)\n",
    "\n",
    "    list_pred = np.array(list_pred)\n",
    "    list_sigmas = np.array(list_sigmas)\n",
    "    nll_i = 0\n",
    "    for ind_sample in range(data_test.shape[0]):\n",
    "        # Convertimos a coordenadas absolutas\n",
    "        displacement = np.cumsum(list_pred[:,ind_sample,:,:], axis=0)\n",
    "        sigmas_abs = np.cumsum(list_sigmas[:,ind_sample,:,:], axis=0)\n",
    "        this_pred_out_abs = displacement + np.array([data_test[ind_sample,:,:][-1].numpy()])\n",
    "\n",
    "        # Recorremos las posiciones\n",
    "        for pos in range(data_test.shape[1]):\n",
    "            g_pdf = []\n",
    "            # Recorremos cada ensemble\n",
    "            for ind in range(num_ensembles):\n",
    "                \n",
    "                g = Gaussian2D( this_pred_out_abs[ind,pos,:] , target_test[ind,pos,:].detach().numpy(), sigmas_abs[ind,pos,:])#.detach().numpy()\n",
    "                g_pdf.append(g)\n",
    "            nll_i += -np.log(np.mean(g_pdf))\n",
    "    nll_batch += nll_i/np.prod(data_test.shape[:2])\n",
    "nll = nll_batch/(batch_idx+1)\n",
    "\n",
    "mean_nll = nll\n",
    "\n",
    "print(\"kde_nll: \", mean_nll)\n",
    "with open('NLL_repeticiones.csv', 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"ensemble\", idTest, mean_nll])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ZSuOG6fQtKL",
    "outputId": "010c12aa-c856-419a-f3eb-e3defe2470e5"
   },
   "outputs": [],
   "source": [
    "### Caso Train ###\n",
    "import time\n",
    "from path_prediction.obstacles import image_to_world_xy\n",
    "from scipy.stats import multivariate_normal\n",
    "import csv\n",
    "\n",
    "#from kde_nll import *\n",
    "\n",
    "#ind_sample = 1\n",
    "#bck = np.load('background.npy')\n",
    "#bck = plt.imread(os.path.join(dataset_dir,dataset_names[idTest],'reference.png'))\n",
    "\n",
    "#seed = 9\n",
    "# Agregamos la semilla\n",
    "#torch.manual_seed(seed)\n",
    "#torch.cuda.manual_seed(seed)\n",
    "\n",
    "nll_batch = 0\n",
    "# Testing\n",
    "for batch_idx, (datarel_test, targetrel_test, data_test, target_test) in enumerate(batched_train_data):\n",
    "    \n",
    "    #for ind_sample in range(data_test.shape[0]):\n",
    "\n",
    "    list_pred = []\n",
    "    list_sigmas = []\n",
    "    # prediction\n",
    "    for ind in range(num_ensembles):\n",
    "\n",
    "        # Cargamos el Modelo\n",
    "        model.load_state_dict(torch.load(\"../training_checkpoints/model_deterministic_\"+str(ind)+\"_\"+str(idTest)+\".pth\",map_location=torch.device('cpu')))\n",
    "        #model.load_state_dict(torch.load(\"model_deterministic/model_deterministic_\"+str(ind)+\"_\"+str(idTest)+\".pth\"))\n",
    "#        model.load_state_dict(torch.load(\"model_deterministic/model_deterministic_0_2.pth\",map_location=torch.device('cpu')))\n",
    "        model.eval()\n",
    "\n",
    "        # Modelo a predecir\n",
    "        #pred, sigmas  = model_drop.predict(datarel_test, dim_pred=12)\n",
    "        #pred, kl, sigmas  = model_var.predict(datarel_test, dim_pred=12)\n",
    "        pred, sigmas = model.predict(datarel_test, dim_pred=12)\n",
    "\n",
    "\n",
    "        list_pred.append(pred)\n",
    "        list_sigmas.append(sigmas)\n",
    "\n",
    "    list_pred = np.array(list_pred)\n",
    "    list_sigmas = np.array(list_sigmas)\n",
    "    nll_i = 0\n",
    "    for ind_sample in range(data_test.shape[0]):\n",
    "        # Convertimos a coordenadas absolutas\n",
    "        displacement = np.cumsum(list_pred[:,ind_sample,:,:], axis=0)\n",
    "        sigmas_abs = np.cumsum(list_sigmas[:,ind_sample,:,:], axis=0)\n",
    "        this_pred_out_abs = displacement + np.array([data_test[ind_sample,:,:][-1].numpy()])\n",
    "\n",
    "        # Recorremos las posiciones\n",
    "        for pos in range(data_test.shape[1]):\n",
    "            g_pdf = []\n",
    "            # Recorremos cada ensemble\n",
    "            for ind in range(num_ensembles):\n",
    "                \n",
    "                g = Gaussian2D( this_pred_out_abs[ind,pos,:] , target_test[ind,pos,:].detach().numpy(), sigmas_abs[ind,pos,:])#.detach().numpy()\n",
    "                g_pdf.append(g)\n",
    "            nll_i += -np.log(np.mean(g_pdf))\n",
    "    nll_batch += nll_i/np.prod(data_test.shape[:2])\n",
    "nll = nll_batch/(batch_idx+1)\n",
    "\n",
    "mean_nll = nll\n",
    "\n",
    "print(\"kde_nll: \", mean_nll)\n",
    "with open('NLL_repeticiones.csv', 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"ensemble\", idTest, mean_nll])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nl0LKtfqRNhT",
    "outputId": "e763d100-e1cf-4c56-af3f-25c359c71145"
   },
   "outputs": [],
   "source": [
    "### Caso Train ###\n",
    "import time\n",
    "from path_prediction.obstacles import image_to_world_xy\n",
    "from scipy.stats import multivariate_normal\n",
    "import csv\n",
    "\n",
    "#from kde_nll import *\n",
    "\n",
    "#ind_sample = 1\n",
    "#bck = np.load('background.npy')\n",
    "#bck = plt.imread(os.path.join(dataset_dir,dataset_names[idTest],'reference.png'))\n",
    "\n",
    "#seed = 9\n",
    "# Agregamos la semilla\n",
    "#torch.manual_seed(seed)\n",
    "#torch.cuda.manual_seed(seed)\n",
    "\n",
    "nll_batch = 0\n",
    "# Testing\n",
    "for batch_idx, (datarel_test, targetrel_test, data_test, target_test) in enumerate(batched_train_data):\n",
    "    \n",
    "    #for ind_sample in range(data_test.shape[0]):\n",
    "\n",
    "    list_pred = []\n",
    "    list_sigmas = []\n",
    "    # prediction\n",
    "    for ind in range(num_ensembles):\n",
    "\n",
    "        # Cargamos el Modelo\n",
    "        model.load_state_dict(torch.load(\"../training_checkpoints/model_deterministic_\"+str(ind)+\"_\"+str(idTest)+\".pth\",map_location=torch.device('cpu')))\n",
    "        #model.load_state_dict(torch.load(\"model_deterministic/model_deterministic_\"+str(ind)+\"_\"+str(idTest)+\".pth\"))\n",
    "#        model.load_state_dict(torch.load(\"model_deterministic/model_deterministic_0_2.pth\",map_location=torch.device('cpu')))\n",
    "        model.eval()\n",
    "\n",
    "        # Modelo a predecir\n",
    "        #pred, sigmas  = model_drop.predict(datarel_test, dim_pred=12)\n",
    "        #pred, kl, sigmas  = model_var.predict(datarel_test, dim_pred=12)\n",
    "        pred, sigmas = model.predict(datarel_test, dim_pred=12)\n",
    "\n",
    "\n",
    "        list_pred.append(pred)\n",
    "        list_sigmas.append(sigmas)\n",
    "\n",
    "    list_pred = np.array(list_pred)\n",
    "    list_sigmas = np.array(list_sigmas)\n",
    "    nll_i = 0\n",
    "    for ind_sample in range(data_test.shape[0]):\n",
    "        # Convertimos a coordenadas absolutas\n",
    "        displacement = np.cumsum(list_pred[:,ind_sample,:,:], axis=0)\n",
    "        sigmas_abs = np.cumsum(list_sigmas[:,ind_sample,:,:], axis=0)\n",
    "        this_pred_out_abs = displacement + np.array([data_test[ind_sample,:,:][-1].numpy()])\n",
    "\n",
    "        # Recorremos las posiciones\n",
    "        for pos in range(data_test.shape[1]):\n",
    "            g_pdf = []\n",
    "            # Recorremos cada ensemble\n",
    "            for ind in range(num_ensembles):\n",
    "                \n",
    "                g = Gaussian2D( this_pred_out_abs[ind,pos,:] , target_test[ind,pos,:].detach().numpy(), sigmas_abs[ind,pos,:])#.detach().numpy()\n",
    "                g_pdf.append(g)\n",
    "            nll_i += -np.log(np.mean(g_pdf))\n",
    "    nll_batch += nll_i/np.prod(data_test.shape[:2])\n",
    "nll = nll_batch/(batch_idx+1)\n",
    "\n",
    "mean_nll = nll\n",
    "\n",
    "print(\"kde_nll: \", mean_nll)\n",
    "with open('NLL_repeticiones.csv', 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"ensemble\", idTest, mean_nll])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xzZSDOx-RNkd",
    "outputId": "5898ff41-dcde-4acd-9773-abfff48d8005"
   },
   "outputs": [],
   "source": [
    "### Caso Train ###\n",
    "import time\n",
    "from path_prediction.obstacles import image_to_world_xy\n",
    "from scipy.stats import multivariate_normal\n",
    "import csv\n",
    "\n",
    "#from kde_nll import *\n",
    "\n",
    "#ind_sample = 1\n",
    "#bck = np.load('background.npy')\n",
    "#bck = plt.imread(os.path.join(dataset_dir,dataset_names[idTest],'reference.png'))\n",
    "\n",
    "#seed = 9\n",
    "# Agregamos la semilla\n",
    "#torch.manual_seed(seed)\n",
    "#torch.cuda.manual_seed(seed)\n",
    "\n",
    "nll_batch = 0\n",
    "# Testing\n",
    "for batch_idx, (datarel_test, targetrel_test, data_test, target_test) in enumerate(batched_train_data):\n",
    "    \n",
    "    #for ind_sample in range(data_test.shape[0]):\n",
    "\n",
    "    list_pred = []\n",
    "    list_sigmas = []\n",
    "    # prediction\n",
    "    for ind in range(num_ensembles):\n",
    "\n",
    "        # Cargamos el Modelo\n",
    "        model.load_state_dict(torch.load(\"../training_checkpoints/model_deterministic_\"+str(ind)+\"_\"+str(idTest)+\".pth\",map_location=torch.device('cpu')))\n",
    "        #model.load_state_dict(torch.load(\"model_deterministic/model_deterministic_\"+str(ind)+\"_\"+str(idTest)+\".pth\"))\n",
    "#        model.load_state_dict(torch.load(\"model_deterministic/model_deterministic_0_2.pth\",map_location=torch.device('cpu')))\n",
    "        model.eval()\n",
    "\n",
    "        # Modelo a predecir\n",
    "        #pred, sigmas  = model_drop.predict(datarel_test, dim_pred=12)\n",
    "        #pred, kl, sigmas  = model_var.predict(datarel_test, dim_pred=12)\n",
    "        pred, sigmas = model.predict(datarel_test, dim_pred=12)\n",
    "\n",
    "\n",
    "        list_pred.append(pred)\n",
    "        list_sigmas.append(sigmas)\n",
    "\n",
    "    list_pred = np.array(list_pred)\n",
    "    list_sigmas = np.array(list_sigmas)\n",
    "    nll_i = 0\n",
    "    for ind_sample in range(data_test.shape[0]):\n",
    "        # Convertimos a coordenadas absolutas\n",
    "        displacement = np.cumsum(list_pred[:,ind_sample,:,:], axis=0)\n",
    "        sigmas_abs = np.cumsum(list_sigmas[:,ind_sample,:,:], axis=0)\n",
    "        this_pred_out_abs = displacement + np.array([data_test[ind_sample,:,:][-1].numpy()])\n",
    "\n",
    "        # Recorremos las posiciones\n",
    "        for pos in range(data_test.shape[1]):\n",
    "            g_pdf = []\n",
    "            # Recorremos cada ensemble\n",
    "            for ind in range(num_ensembles):\n",
    "                \n",
    "                g = Gaussian2D( this_pred_out_abs[ind,pos,:] , target_test[ind,pos,:].detach().numpy(), sigmas_abs[ind,pos,:])#.detach().numpy()\n",
    "                g_pdf.append(g)\n",
    "            nll_i += -np.log(np.mean(g_pdf))\n",
    "    nll_batch += nll_i/np.prod(data_test.shape[:2])\n",
    "nll = nll_batch/(batch_idx+1)\n",
    "\n",
    "mean_nll = nll\n",
    "\n",
    "print(\"kde_nll: \", mean_nll)\n",
    "with open('NLL_repeticiones.csv', 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"ensemble\", idTest, mean_nll])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fzBD8P6FRNne",
    "outputId": "c8c9287a-958a-4324-cc91-6248c0dfa1ec"
   },
   "outputs": [],
   "source": [
    "### Caso Train ###\n",
    "import time\n",
    "from path_prediction.obstacles import image_to_world_xy\n",
    "from scipy.stats import multivariate_normal\n",
    "import csv\n",
    "\n",
    "#from kde_nll import *\n",
    "\n",
    "#ind_sample = 1\n",
    "#bck = np.load('background.npy')\n",
    "#bck = plt.imread(os.path.join(dataset_dir,dataset_names[idTest],'reference.png'))\n",
    "\n",
    "#seed = 9\n",
    "# Agregamos la semilla\n",
    "#torch.manual_seed(seed)\n",
    "#torch.cuda.manual_seed(seed)\n",
    "\n",
    "nll_batch = 0\n",
    "# Testing\n",
    "for batch_idx, (datarel_test, targetrel_test, data_test, target_test) in enumerate(batched_train_data):\n",
    "    \n",
    "    #for ind_sample in range(data_test.shape[0]):\n",
    "\n",
    "    list_pred = []\n",
    "    list_sigmas = []\n",
    "    # prediction\n",
    "    for ind in range(num_ensembles):\n",
    "\n",
    "        # Cargamos el Modelo\n",
    "        model.load_state_dict(torch.load(\"../training_checkpoints/model_deterministic_\"+str(ind)+\"_\"+str(idTest)+\".pth\",map_location=torch.device('cpu')))\n",
    "        #model.load_state_dict(torch.load(\"model_deterministic/model_deterministic_\"+str(ind)+\"_\"+str(idTest)+\".pth\"))\n",
    "#        model.load_state_dict(torch.load(\"model_deterministic/model_deterministic_0_2.pth\",map_location=torch.device('cpu')))\n",
    "        model.eval()\n",
    "\n",
    "        # Modelo a predecir\n",
    "        #pred, sigmas  = model_drop.predict(datarel_test, dim_pred=12)\n",
    "        #pred, kl, sigmas  = model_var.predict(datarel_test, dim_pred=12)\n",
    "        pred, sigmas = model.predict(datarel_test, dim_pred=12)\n",
    "\n",
    "\n",
    "        list_pred.append(pred)\n",
    "        list_sigmas.append(sigmas)\n",
    "\n",
    "    list_pred = np.array(list_pred)\n",
    "    list_sigmas = np.array(list_sigmas)\n",
    "    nll_i = 0\n",
    "    for ind_sample in range(data_test.shape[0]):\n",
    "        # Convertimos a coordenadas absolutas\n",
    "        displacement = np.cumsum(list_pred[:,ind_sample,:,:], axis=0)\n",
    "        sigmas_abs = np.cumsum(list_sigmas[:,ind_sample,:,:], axis=0)\n",
    "        this_pred_out_abs = displacement + np.array([data_test[ind_sample,:,:][-1].numpy()])\n",
    "\n",
    "        # Recorremos las posiciones\n",
    "        for pos in range(data_test.shape[1]):\n",
    "            g_pdf = []\n",
    "            # Recorremos cada ensemble\n",
    "            for ind in range(num_ensembles):\n",
    "                \n",
    "                g = Gaussian2D( this_pred_out_abs[ind,pos,:] , target_test[ind,pos,:].detach().numpy(), sigmas_abs[ind,pos,:])#.detach().numpy()\n",
    "                g_pdf.append(g)\n",
    "            nll_i += -np.log(np.mean(g_pdf))\n",
    "    nll_batch += nll_i/np.prod(data_test.shape[:2])\n",
    "nll = nll_batch/(batch_idx+1)\n",
    "\n",
    "mean_nll = nll\n",
    "\n",
    "print(\"kde_nll: \", mean_nll)\n",
    "with open('NLL_repeticiones.csv', 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"ensemble\", idTest, mean_nll])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asp2ZVLyUJ2a"
   },
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P8XcdNi_RNqX",
    "outputId": "deb40fb1-6826-402d-8e76-57dc04dd7f5d"
   },
   "outputs": [],
   "source": [
    "### Caso Train ###\n",
    "import time\n",
    "from path_prediction.obstacles import image_to_world_xy\n",
    "from scipy.stats import multivariate_normal\n",
    "import csv\n",
    "\n",
    "#from kde_nll import *\n",
    "\n",
    "#ind_sample = 1\n",
    "#bck = np.load('background.npy')\n",
    "#bck = plt.imread(os.path.join(dataset_dir,dataset_names[idTest],'reference.png'))\n",
    "\n",
    "#seed = 9\n",
    "# Agregamos la semilla\n",
    "#torch.manual_seed(seed)\n",
    "#torch.cuda.manual_seed(seed)\n",
    "\n",
    "nll_batch = 0\n",
    "# Testing\n",
    "for batch_idx, (datarel_test, targetrel_test, data_test, target_test) in enumerate(batched_train_data):\n",
    "    \n",
    "    #for ind_sample in range(data_test.shape[0]):\n",
    "\n",
    "    list_pred = []\n",
    "    list_sigmas = []\n",
    "    # prediction\n",
    "    for ind in range(num_ensembles):\n",
    "\n",
    "        # Modelo a predecir\n",
    "        pred, sigmas  = model_drop.predict(datarel_test, dim_pred=12)\n",
    "        #pred, kl, sigmas  = model_var.predict(datarel_test, dim_pred=12)\n",
    "        #pred, sigmas = model.predict(datarel_test, dim_pred=12)\n",
    "\n",
    "\n",
    "        list_pred.append(pred)\n",
    "        list_sigmas.append(sigmas)\n",
    "\n",
    "    list_pred = np.array(list_pred)\n",
    "    list_sigmas = np.array(list_sigmas)\n",
    "    nll_i = 0\n",
    "    for ind_sample in range(data_test.shape[0]):\n",
    "        # Convertimos a coordenadas absolutas\n",
    "        displacement = np.cumsum(list_pred[:,ind_sample,:,:], axis=0)\n",
    "        sigmas_abs = np.cumsum(list_sigmas[:,ind_sample,:,:], axis=0)\n",
    "        this_pred_out_abs = displacement + np.array([data_test[ind_sample,:,:][-1].numpy()])\n",
    "\n",
    "        # Recorremos las posiciones\n",
    "        for pos in range(data_test.shape[1]):\n",
    "            g_pdf = []\n",
    "            # Recorremos cada ensemble\n",
    "            for ind in range(num_ensembles):\n",
    "                \n",
    "                g = Gaussian2D( this_pred_out_abs[ind,pos,:] , target_test[ind,pos,:].detach().numpy(), sigmas_abs[ind,pos,:])#.detach().numpy()\n",
    "                g_pdf.append(g)\n",
    "            nll_i += -np.log(np.mean(g_pdf))\n",
    "    nll_batch += nll_i/np.prod(data_test.shape[:2])\n",
    "nll = nll_batch/(batch_idx+1)\n",
    "\n",
    "mean_nll = nll\n",
    "\n",
    "print(\"kde_nll: \", mean_nll)\n",
    "with open('NLL_repeticiones.csv', 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"dropout\", idTest, mean_nll])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NMzznFxCRNs_",
    "outputId": "82a3a88d-1661-4232-d6a2-284af2b10872"
   },
   "outputs": [],
   "source": [
    "### Caso Train ###\n",
    "import time\n",
    "from path_prediction.obstacles import image_to_world_xy\n",
    "from scipy.stats import multivariate_normal\n",
    "import csv\n",
    "\n",
    "#from kde_nll import *\n",
    "\n",
    "#ind_sample = 1\n",
    "#bck = np.load('background.npy')\n",
    "#bck = plt.imread(os.path.join(dataset_dir,dataset_names[idTest],'reference.png'))\n",
    "\n",
    "#seed = 9\n",
    "# Agregamos la semilla\n",
    "#torch.manual_seed(seed)\n",
    "#torch.cuda.manual_seed(seed)\n",
    "\n",
    "nll_batch = 0\n",
    "# Testing\n",
    "for batch_idx, (datarel_test, targetrel_test, data_test, target_test) in enumerate(batched_train_data):\n",
    "    \n",
    "    #for ind_sample in range(data_test.shape[0]):\n",
    "\n",
    "    list_pred = []\n",
    "    list_sigmas = []\n",
    "    # prediction\n",
    "    for ind in range(num_ensembles):\n",
    "\n",
    "        # Modelo a predecir\n",
    "        pred, sigmas  = model_drop.predict(datarel_test, dim_pred=12)\n",
    "        #pred, kl, sigmas  = model_var.predict(datarel_test, dim_pred=12)\n",
    "        #pred, sigmas = model.predict(datarel_test, dim_pred=12)\n",
    "\n",
    "\n",
    "        list_pred.append(pred)\n",
    "        list_sigmas.append(sigmas)\n",
    "\n",
    "    list_pred = np.array(list_pred)\n",
    "    list_sigmas = np.array(list_sigmas)\n",
    "    nll_i = 0\n",
    "    for ind_sample in range(data_test.shape[0]):\n",
    "        # Convertimos a coordenadas absolutas\n",
    "        displacement = np.cumsum(list_pred[:,ind_sample,:,:], axis=0)\n",
    "        sigmas_abs = np.cumsum(list_sigmas[:,ind_sample,:,:], axis=0)\n",
    "        this_pred_out_abs = displacement + np.array([data_test[ind_sample,:,:][-1].numpy()])\n",
    "\n",
    "        # Recorremos las posiciones\n",
    "        for pos in range(data_test.shape[1]):\n",
    "            g_pdf = []\n",
    "            # Recorremos cada ensemble\n",
    "            for ind in range(num_ensembles):\n",
    "                \n",
    "                g = Gaussian2D( this_pred_out_abs[ind,pos,:] , target_test[ind,pos,:].detach().numpy(), sigmas_abs[ind,pos,:])#.detach().numpy()\n",
    "                g_pdf.append(g)\n",
    "            nll_i += -np.log(np.mean(g_pdf))\n",
    "    nll_batch += nll_i/np.prod(data_test.shape[:2])\n",
    "nll = nll_batch/(batch_idx+1)\n",
    "\n",
    "mean_nll = nll\n",
    "\n",
    "print(\"kde_nll: \", mean_nll)\n",
    "with open('NLL_repeticiones.csv', 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"dropout\", idTest, mean_nll])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ubrKlORUUpb",
    "outputId": "e7edb7b9-a004-48cb-b5e0-68e604734e10"
   },
   "outputs": [],
   "source": [
    "### Caso Train ###\n",
    "import time\n",
    "from path_prediction.obstacles import image_to_world_xy\n",
    "from scipy.stats import multivariate_normal\n",
    "import csv\n",
    "\n",
    "#from kde_nll import *\n",
    "\n",
    "#ind_sample = 1\n",
    "#bck = np.load('background.npy')\n",
    "#bck = plt.imread(os.path.join(dataset_dir,dataset_names[idTest],'reference.png'))\n",
    "\n",
    "#seed = 9\n",
    "# Agregamos la semilla\n",
    "#torch.manual_seed(seed)\n",
    "#torch.cuda.manual_seed(seed)\n",
    "\n",
    "nll_batch = 0\n",
    "# Testing\n",
    "for batch_idx, (datarel_test, targetrel_test, data_test, target_test) in enumerate(batched_train_data):\n",
    "    \n",
    "    #for ind_sample in range(data_test.shape[0]):\n",
    "\n",
    "    list_pred = []\n",
    "    list_sigmas = []\n",
    "    # prediction\n",
    "    for ind in range(num_ensembles):\n",
    "\n",
    "        # Modelo a predecir\n",
    "        pred, sigmas  = model_drop.predict(datarel_test, dim_pred=12)\n",
    "        #pred, kl, sigmas  = model_var.predict(datarel_test, dim_pred=12)\n",
    "        #pred, sigmas = model.predict(datarel_test, dim_pred=12)\n",
    "\n",
    "\n",
    "        list_pred.append(pred)\n",
    "        list_sigmas.append(sigmas)\n",
    "\n",
    "    list_pred = np.array(list_pred)\n",
    "    list_sigmas = np.array(list_sigmas)\n",
    "    nll_i = 0\n",
    "    for ind_sample in range(data_test.shape[0]):\n",
    "        # Convertimos a coordenadas absolutas\n",
    "        displacement = np.cumsum(list_pred[:,ind_sample,:,:], axis=0)\n",
    "        sigmas_abs = np.cumsum(list_sigmas[:,ind_sample,:,:], axis=0)\n",
    "        this_pred_out_abs = displacement + np.array([data_test[ind_sample,:,:][-1].numpy()])\n",
    "\n",
    "        # Recorremos las posiciones\n",
    "        for pos in range(data_test.shape[1]):\n",
    "            g_pdf = []\n",
    "            # Recorremos cada ensemble\n",
    "            for ind in range(num_ensembles):\n",
    "                \n",
    "                g = Gaussian2D( this_pred_out_abs[ind,pos,:] , target_test[ind,pos,:].detach().numpy(), sigmas_abs[ind,pos,:])#.detach().numpy()\n",
    "                g_pdf.append(g)\n",
    "            nll_i += -np.log(np.mean(g_pdf))\n",
    "    nll_batch += nll_i/np.prod(data_test.shape[:2])\n",
    "nll = nll_batch/(batch_idx+1)\n",
    "\n",
    "mean_nll = nll\n",
    "\n",
    "print(\"kde_nll: \", mean_nll)\n",
    "with open('NLL_repeticiones.csv', 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"dropout\", idTest, mean_nll])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q4ckW4IsUUsZ",
    "outputId": "2b77f8fc-e8ab-4234-8141-7fa1d7af064d"
   },
   "outputs": [],
   "source": [
    "### Caso Train ###\n",
    "import time\n",
    "from path_prediction.obstacles import image_to_world_xy\n",
    "from scipy.stats import multivariate_normal\n",
    "import csv\n",
    "\n",
    "#from kde_nll import *\n",
    "\n",
    "#ind_sample = 1\n",
    "#bck = np.load('background.npy')\n",
    "#bck = plt.imread(os.path.join(dataset_dir,dataset_names[idTest],'reference.png'))\n",
    "\n",
    "#seed = 9\n",
    "# Agregamos la semilla\n",
    "#torch.manual_seed(seed)\n",
    "#torch.cuda.manual_seed(seed)\n",
    "\n",
    "nll_batch = 0\n",
    "# Testing\n",
    "for batch_idx, (datarel_test, targetrel_test, data_test, target_test) in enumerate(batched_train_data):\n",
    "    \n",
    "    #for ind_sample in range(data_test.shape[0]):\n",
    "\n",
    "    list_pred = []\n",
    "    list_sigmas = []\n",
    "    # prediction\n",
    "    for ind in range(num_ensembles):\n",
    "\n",
    "        # Modelo a predecir\n",
    "        pred, sigmas  = model_drop.predict(datarel_test, dim_pred=12)\n",
    "        #pred, kl, sigmas  = model_var.predict(datarel_test, dim_pred=12)\n",
    "        #pred, sigmas = model.predict(datarel_test, dim_pred=12)\n",
    "\n",
    "\n",
    "        list_pred.append(pred)\n",
    "        list_sigmas.append(sigmas)\n",
    "\n",
    "    list_pred = np.array(list_pred)\n",
    "    list_sigmas = np.array(list_sigmas)\n",
    "    nll_i = 0\n",
    "    for ind_sample in range(data_test.shape[0]):\n",
    "        # Convertimos a coordenadas absolutas\n",
    "        displacement = np.cumsum(list_pred[:,ind_sample,:,:], axis=0)\n",
    "        sigmas_abs = np.cumsum(list_sigmas[:,ind_sample,:,:], axis=0)\n",
    "        this_pred_out_abs = displacement + np.array([data_test[ind_sample,:,:][-1].numpy()])\n",
    "\n",
    "        # Recorremos las posiciones\n",
    "        for pos in range(data_test.shape[1]):\n",
    "            g_pdf = []\n",
    "            # Recorremos cada ensemble\n",
    "            for ind in range(num_ensembles):\n",
    "                \n",
    "                g = Gaussian2D( this_pred_out_abs[ind,pos,:] , target_test[ind,pos,:].detach().numpy(), sigmas_abs[ind,pos,:])#.detach().numpy()\n",
    "                g_pdf.append(g)\n",
    "            nll_i += -np.log(np.mean(g_pdf))\n",
    "    nll_batch += nll_i/np.prod(data_test.shape[:2])\n",
    "nll = nll_batch/(batch_idx+1)\n",
    "\n",
    "mean_nll = nll\n",
    "\n",
    "print(\"kde_nll: \", mean_nll)\n",
    "with open('NLL_repeticiones.csv', 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"dropout\", idTest, mean_nll])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8kgIW8vyUUvX",
    "outputId": "d0cbfab1-38eb-4097-bee1-9f57a1e365b6"
   },
   "outputs": [],
   "source": [
    "### Caso Train ###\n",
    "import time\n",
    "from path_prediction.obstacles import image_to_world_xy\n",
    "from scipy.stats import multivariate_normal\n",
    "import csv\n",
    "\n",
    "#from kde_nll import *\n",
    "\n",
    "#ind_sample = 1\n",
    "#bck = np.load('background.npy')\n",
    "#bck = plt.imread(os.path.join(dataset_dir,dataset_names[idTest],'reference.png'))\n",
    "\n",
    "#seed = 9\n",
    "# Agregamos la semilla\n",
    "#torch.manual_seed(seed)\n",
    "#torch.cuda.manual_seed(seed)\n",
    "\n",
    "nll_batch = 0\n",
    "# Testing\n",
    "for batch_idx, (datarel_test, targetrel_test, data_test, target_test) in enumerate(batched_train_data):\n",
    "    \n",
    "    #for ind_sample in range(data_test.shape[0]):\n",
    "\n",
    "    list_pred = []\n",
    "    list_sigmas = []\n",
    "    # prediction\n",
    "    for ind in range(num_ensembles):\n",
    "\n",
    "        # Modelo a predecir\n",
    "        pred, sigmas  = model_drop.predict(datarel_test, dim_pred=12)\n",
    "        #pred, kl, sigmas  = model_var.predict(datarel_test, dim_pred=12)\n",
    "        #pred, sigmas = model.predict(datarel_test, dim_pred=12)\n",
    "\n",
    "\n",
    "        list_pred.append(pred)\n",
    "        list_sigmas.append(sigmas)\n",
    "\n",
    "    list_pred = np.array(list_pred)\n",
    "    list_sigmas = np.array(list_sigmas)\n",
    "    nll_i = 0\n",
    "    for ind_sample in range(data_test.shape[0]):\n",
    "        # Convertimos a coordenadas absolutas\n",
    "        displacement = np.cumsum(list_pred[:,ind_sample,:,:], axis=0)\n",
    "        sigmas_abs = np.cumsum(list_sigmas[:,ind_sample,:,:], axis=0)\n",
    "        this_pred_out_abs = displacement + np.array([data_test[ind_sample,:,:][-1].numpy()])\n",
    "\n",
    "        # Recorremos las posiciones\n",
    "        for pos in range(data_test.shape[1]):\n",
    "            g_pdf = []\n",
    "            # Recorremos cada ensemble\n",
    "            for ind in range(num_ensembles):\n",
    "                \n",
    "                g = Gaussian2D( this_pred_out_abs[ind,pos,:] , target_test[ind,pos,:].detach().numpy(), sigmas_abs[ind,pos,:])#.detach().numpy()\n",
    "                g_pdf.append(g)\n",
    "            nll_i += -np.log(np.mean(g_pdf))\n",
    "    nll_batch += nll_i/np.prod(data_test.shape[:2])\n",
    "nll = nll_batch/(batch_idx+1)\n",
    "\n",
    "mean_nll = nll\n",
    "\n",
    "print(\"kde_nll: \", mean_nll)\n",
    "with open('NLL_repeticiones.csv', 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"dropout\", idTest, mean_nll])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5iCKVOEUUx5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4WsNsqBUXbh"
   },
   "source": [
    "### Variational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oXklR6fOUU0h",
    "outputId": "aac106ed-71f3-4757-a2e3-1ccd50efb6bf"
   },
   "outputs": [],
   "source": [
    "### Caso Train ###\n",
    "import time\n",
    "from path_prediction.obstacles import image_to_world_xy\n",
    "from scipy.stats import multivariate_normal\n",
    "import csv\n",
    "\n",
    "#from kde_nll import *\n",
    "\n",
    "#ind_sample = 1\n",
    "#bck = np.load('background.npy')\n",
    "#bck = plt.imread(os.path.join(dataset_dir,dataset_names[idTest],'reference.png'))\n",
    "\n",
    "#seed = 9\n",
    "# Agregamos la semilla\n",
    "#torch.manual_seed(seed)\n",
    "#torch.cuda.manual_seed(seed)\n",
    "\n",
    "nll_batch = 0\n",
    "# Testing\n",
    "for batch_idx, (datarel_test, targetrel_test, data_test, target_test) in enumerate(batched_train_data):\n",
    "    \n",
    "    #for ind_sample in range(data_test.shape[0]):\n",
    "\n",
    "    list_pred = []\n",
    "    list_sigmas = []\n",
    "    # prediction\n",
    "    for ind in range(num_ensembles):\n",
    "\n",
    "        # Modelo a predecir\n",
    "        #pred, sigmas  = model_drop.predict(datarel_test, dim_pred=12)\n",
    "        pred, kl, sigmas  = model_var.predict(datarel_test, dim_pred=12)\n",
    "        #pred, sigmas = model.predict(datarel_test, dim_pred=12)\n",
    "\n",
    "\n",
    "        list_pred.append(pred)\n",
    "        list_sigmas.append(sigmas)\n",
    "\n",
    "    list_pred = np.array(list_pred)\n",
    "    list_sigmas = np.array(list_sigmas)\n",
    "    nll_i = 0\n",
    "    for ind_sample in range(data_test.shape[0]):\n",
    "        # Convertimos a coordenadas absolutas\n",
    "        displacement = np.cumsum(list_pred[:,ind_sample,:,:], axis=0)\n",
    "        sigmas_abs = np.cumsum(list_sigmas[:,ind_sample,:,:], axis=0)\n",
    "        this_pred_out_abs = displacement + np.array([data_test[ind_sample,:,:][-1].numpy()])\n",
    "\n",
    "        # Recorremos las posiciones\n",
    "        for pos in range(data_test.shape[1]):\n",
    "            g_pdf = []\n",
    "            # Recorremos cada ensemble\n",
    "            for ind in range(num_ensembles):\n",
    "                \n",
    "                g = Gaussian2D( this_pred_out_abs[ind,pos,:] , target_test[ind,pos,:].detach().numpy(), sigmas_abs[ind,pos,:])#.detach().numpy()\n",
    "                g_pdf.append(g)\n",
    "            nll_i += -np.log(np.mean(g_pdf))\n",
    "    nll_batch += nll_i/np.prod(data_test.shape[:2])\n",
    "nll = nll_batch/(batch_idx+1)\n",
    "\n",
    "mean_nll = nll\n",
    "\n",
    "print(\"kde_nll: \", mean_nll)\n",
    "with open('NLL_repeticiones.csv', 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"variational\", idTest, mean_nll])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nZVWc7rxUU3G",
    "outputId": "276e8c1b-bdd3-4a5e-e5f6-1c40032bc1ea"
   },
   "outputs": [],
   "source": [
    "### Caso Train ###\n",
    "import time\n",
    "from path_prediction.obstacles import image_to_world_xy\n",
    "from scipy.stats import multivariate_normal\n",
    "import csv\n",
    "\n",
    "#from kde_nll import *\n",
    "\n",
    "#ind_sample = 1\n",
    "#bck = np.load('background.npy')\n",
    "#bck = plt.imread(os.path.join(dataset_dir,dataset_names[idTest],'reference.png'))\n",
    "\n",
    "#seed = 9\n",
    "# Agregamos la semilla\n",
    "#torch.manual_seed(seed)\n",
    "#torch.cuda.manual_seed(seed)\n",
    "\n",
    "nll_batch = 0\n",
    "# Testing\n",
    "for batch_idx, (datarel_test, targetrel_test, data_test, target_test) in enumerate(batched_train_data):\n",
    "    \n",
    "    #for ind_sample in range(data_test.shape[0]):\n",
    "\n",
    "    list_pred = []\n",
    "    list_sigmas = []\n",
    "    # prediction\n",
    "    for ind in range(num_ensembles):\n",
    "\n",
    "        # Modelo a predecir\n",
    "        #pred, sigmas  = model_drop.predict(datarel_test, dim_pred=12)\n",
    "        pred, kl, sigmas  = model_var.predict(datarel_test, dim_pred=12)\n",
    "        #pred, sigmas = model.predict(datarel_test, dim_pred=12)\n",
    "\n",
    "\n",
    "        list_pred.append(pred)\n",
    "        list_sigmas.append(sigmas)\n",
    "\n",
    "    list_pred = np.array(list_pred)\n",
    "    list_sigmas = np.array(list_sigmas)\n",
    "    nll_i = 0\n",
    "    for ind_sample in range(data_test.shape[0]):\n",
    "        # Convertimos a coordenadas absolutas\n",
    "        displacement = np.cumsum(list_pred[:,ind_sample,:,:], axis=0)\n",
    "        sigmas_abs = np.cumsum(list_sigmas[:,ind_sample,:,:], axis=0)\n",
    "        this_pred_out_abs = displacement + np.array([data_test[ind_sample,:,:][-1].numpy()])\n",
    "\n",
    "        # Recorremos las posiciones\n",
    "        for pos in range(data_test.shape[1]):\n",
    "            g_pdf = []\n",
    "            # Recorremos cada ensemble\n",
    "            for ind in range(num_ensembles):\n",
    "                \n",
    "                g = Gaussian2D( this_pred_out_abs[ind,pos,:] , target_test[ind,pos,:].detach().numpy(), sigmas_abs[ind,pos,:])#.detach().numpy()\n",
    "                g_pdf.append(g)\n",
    "            nll_i += -np.log(np.mean(g_pdf))\n",
    "    nll_batch += nll_i/np.prod(data_test.shape[:2])\n",
    "nll = nll_batch/(batch_idx+1)\n",
    "\n",
    "mean_nll = nll\n",
    "\n",
    "print(\"kde_nll: \", mean_nll)\n",
    "with open('NLL_repeticiones.csv', 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"variational\", idTest, mean_nll])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pYV7XAVoUU6G",
    "outputId": "fb513a23-1dd2-4645-8ff5-9777a9761bf1"
   },
   "outputs": [],
   "source": [
    "### Caso Train ###\n",
    "import time\n",
    "from path_prediction.obstacles import image_to_world_xy\n",
    "from scipy.stats import multivariate_normal\n",
    "import csv\n",
    "\n",
    "#from kde_nll import *\n",
    "\n",
    "#ind_sample = 1\n",
    "#bck = np.load('background.npy')\n",
    "#bck = plt.imread(os.path.join(dataset_dir,dataset_names[idTest],'reference.png'))\n",
    "\n",
    "#seed = 9\n",
    "# Agregamos la semilla\n",
    "#torch.manual_seed(seed)\n",
    "#torch.cuda.manual_seed(seed)\n",
    "\n",
    "nll_batch = 0\n",
    "# Testing\n",
    "for batch_idx, (datarel_test, targetrel_test, data_test, target_test) in enumerate(batched_train_data):\n",
    "    \n",
    "    #for ind_sample in range(data_test.shape[0]):\n",
    "\n",
    "    list_pred = []\n",
    "    list_sigmas = []\n",
    "    # prediction\n",
    "    for ind in range(num_ensembles):\n",
    "\n",
    "        # Modelo a predecir\n",
    "        #pred, sigmas  = model_drop.predict(datarel_test, dim_pred=12)\n",
    "        pred, kl, sigmas  = model_var.predict(datarel_test, dim_pred=12)\n",
    "        #pred, sigmas = model.predict(datarel_test, dim_pred=12)\n",
    "\n",
    "\n",
    "        list_pred.append(pred)\n",
    "        list_sigmas.append(sigmas)\n",
    "\n",
    "    list_pred = np.array(list_pred)\n",
    "    list_sigmas = np.array(list_sigmas)\n",
    "    nll_i = 0\n",
    "    for ind_sample in range(data_test.shape[0]):\n",
    "        # Convertimos a coordenadas absolutas\n",
    "        displacement = np.cumsum(list_pred[:,ind_sample,:,:], axis=0)\n",
    "        sigmas_abs = np.cumsum(list_sigmas[:,ind_sample,:,:], axis=0)\n",
    "        this_pred_out_abs = displacement + np.array([data_test[ind_sample,:,:][-1].numpy()])\n",
    "\n",
    "        # Recorremos las posiciones\n",
    "        for pos in range(data_test.shape[1]):\n",
    "            g_pdf = []\n",
    "            # Recorremos cada ensemble\n",
    "            for ind in range(num_ensembles):\n",
    "                \n",
    "                g = Gaussian2D( this_pred_out_abs[ind,pos,:] , target_test[ind,pos,:].detach().numpy(), sigmas_abs[ind,pos,:])#.detach().numpy()\n",
    "                g_pdf.append(g)\n",
    "            nll_i += -np.log(np.mean(g_pdf))\n",
    "    nll_batch += nll_i/np.prod(data_test.shape[:2])\n",
    "nll = nll_batch/(batch_idx+1)\n",
    "\n",
    "mean_nll = nll\n",
    "\n",
    "print(\"kde_nll: \", mean_nll)\n",
    "with open('NLL_repeticiones.csv', 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"variational\", idTest, mean_nll])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pu0PjNtDRNwB",
    "outputId": "08da3cbb-5a23-4e3a-f673-498aca662e90"
   },
   "outputs": [],
   "source": [
    "### Caso Train ###\n",
    "import time\n",
    "from path_prediction.obstacles import image_to_world_xy\n",
    "from scipy.stats import multivariate_normal\n",
    "import csv\n",
    "\n",
    "#from kde_nll import *\n",
    "\n",
    "#ind_sample = 1\n",
    "#bck = np.load('background.npy')\n",
    "#bck = plt.imread(os.path.join(dataset_dir,dataset_names[idTest],'reference.png'))\n",
    "\n",
    "#seed = 9\n",
    "# Agregamos la semilla\n",
    "#torch.manual_seed(seed)\n",
    "#torch.cuda.manual_seed(seed)\n",
    "\n",
    "nll_batch = 0\n",
    "# Testing\n",
    "for batch_idx, (datarel_test, targetrel_test, data_test, target_test) in enumerate(batched_train_data):\n",
    "    \n",
    "    #for ind_sample in range(data_test.shape[0]):\n",
    "\n",
    "    list_pred = []\n",
    "    list_sigmas = []\n",
    "    # prediction\n",
    "    for ind in range(num_ensembles):\n",
    "\n",
    "        # Modelo a predecir\n",
    "        #pred, sigmas  = model_drop.predict(datarel_test, dim_pred=12)\n",
    "        pred, kl, sigmas  = model_var.predict(datarel_test, dim_pred=12)\n",
    "        #pred, sigmas = model.predict(datarel_test, dim_pred=12)\n",
    "\n",
    "\n",
    "        list_pred.append(pred)\n",
    "        list_sigmas.append(sigmas)\n",
    "\n",
    "    list_pred = np.array(list_pred)\n",
    "    list_sigmas = np.array(list_sigmas)\n",
    "    nll_i = 0\n",
    "    for ind_sample in range(data_test.shape[0]):\n",
    "        # Convertimos a coordenadas absolutas\n",
    "        displacement = np.cumsum(list_pred[:,ind_sample,:,:], axis=0)\n",
    "        sigmas_abs = np.cumsum(list_sigmas[:,ind_sample,:,:], axis=0)\n",
    "        this_pred_out_abs = displacement + np.array([data_test[ind_sample,:,:][-1].numpy()])\n",
    "\n",
    "        # Recorremos las posiciones\n",
    "        for pos in range(data_test.shape[1]):\n",
    "            g_pdf = []\n",
    "            # Recorremos cada ensemble\n",
    "            for ind in range(num_ensembles):\n",
    "                \n",
    "                g = Gaussian2D( this_pred_out_abs[ind,pos,:] , target_test[ind,pos,:].detach().numpy(), sigmas_abs[ind,pos,:])#.detach().numpy()\n",
    "                g_pdf.append(g)\n",
    "            nll_i += -np.log(np.mean(g_pdf))\n",
    "    nll_batch += nll_i/np.prod(data_test.shape[:2])\n",
    "nll = nll_batch/(batch_idx+1)\n",
    "\n",
    "mean_nll = nll\n",
    "\n",
    "print(\"kde_nll: \", mean_nll)\n",
    "with open('NLL_repeticiones.csv', 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"variational\", idTest, mean_nll])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PTHFFk6pRNzL",
    "outputId": "7238a944-ae5a-4e48-ddde-a8f8574ba006"
   },
   "outputs": [],
   "source": [
    "### Caso Train ###\n",
    "import time\n",
    "from path_prediction.obstacles import image_to_world_xy\n",
    "from scipy.stats import multivariate_normal\n",
    "import csv\n",
    "\n",
    "#from kde_nll import *\n",
    "\n",
    "#ind_sample = 1\n",
    "#bck = np.load('background.npy')\n",
    "#bck = plt.imread(os.path.join(dataset_dir,dataset_names[idTest],'reference.png'))\n",
    "\n",
    "#seed = 9\n",
    "# Agregamos la semilla\n",
    "#torch.manual_seed(seed)\n",
    "#torch.cuda.manual_seed(seed)\n",
    "\n",
    "nll_batch = 0\n",
    "# Testing\n",
    "for batch_idx, (datarel_test, targetrel_test, data_test, target_test) in enumerate(batched_train_data):\n",
    "    \n",
    "    #for ind_sample in range(data_test.shape[0]):\n",
    "\n",
    "    list_pred = []\n",
    "    list_sigmas = []\n",
    "    # prediction\n",
    "    for ind in range(num_ensembles):\n",
    "\n",
    "        # Modelo a predecir\n",
    "        #pred, sigmas  = model_drop.predict(datarel_test, dim_pred=12)\n",
    "        pred, kl, sigmas  = model_var.predict(datarel_test, dim_pred=12)\n",
    "        #pred, sigmas = model.predict(datarel_test, dim_pred=12)\n",
    "\n",
    "\n",
    "        list_pred.append(pred)\n",
    "        list_sigmas.append(sigmas)\n",
    "\n",
    "    list_pred = np.array(list_pred)\n",
    "    list_sigmas = np.array(list_sigmas)\n",
    "    nll_i = 0\n",
    "    for ind_sample in range(data_test.shape[0]):\n",
    "        # Convertimos a coordenadas absolutas\n",
    "        displacement = np.cumsum(list_pred[:,ind_sample,:,:], axis=0)\n",
    "        sigmas_abs = np.cumsum(list_sigmas[:,ind_sample,:,:], axis=0)\n",
    "        this_pred_out_abs = displacement + np.array([data_test[ind_sample,:,:][-1].numpy()])\n",
    "\n",
    "        # Recorremos las posiciones\n",
    "        for pos in range(data_test.shape[1]):\n",
    "            g_pdf = []\n",
    "            # Recorremos cada ensemble\n",
    "            for ind in range(num_ensembles):\n",
    "                \n",
    "                g = Gaussian2D( this_pred_out_abs[ind,pos,:] , target_test[ind,pos,:].detach().numpy(), sigmas_abs[ind,pos,:])#.detach().numpy()\n",
    "                g_pdf.append(g)\n",
    "            nll_i += -np.log(np.mean(g_pdf))\n",
    "    nll_batch += nll_i/np.prod(data_test.shape[:2])\n",
    "nll = nll_batch/(batch_idx+1)\n",
    "\n",
    "mean_nll = nll\n",
    "\n",
    "print(\"kde_nll: \", mean_nll)\n",
    "with open('NLL_repeticiones.csv', 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"variational\", idTest, mean_nll])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "puYkkG9nUfHS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajYIJ8llUfKR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBWhmDFTUfNT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0wIpZ_IUfP9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aux0-QBpUfSm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLL_repeticiones.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
